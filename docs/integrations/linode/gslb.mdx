---
description: In this guide, you'll learn how to layer load balancing between three or more globbly-distributed, cloud-based virtual machines in Linode.
---

# Use ngrok's Global Server Load Balancing with Linode

:::tip TL;DR

To use ngrok's [Global Server Load Balancing](https://ngrok.com/docs/network-edge/gslb/) with Linode:

1. [Reserve your ngrok domain](#reserve-domain)
2. [Create your ngrok Edge](#create-edge)
3. [Create Tunnel Group backends for your VMs](#create-tunnel-group-backends)
4. [Create a Weighted Backend and Route on your Edge](#create-weighted-backend)
5. [Install the ngrok agent and an example workload on each VM](#install-ngrok-agent)
6. [Test out ngrok's Global Server Load Balancing](#test-gslb)
7. [(optional) Enable the Traffic Policy module for API gateway features](#enable-traffic-policy)

:::

In this guide, you'll learn how to layer ngrok's [Global Server Load Balancing](https://ngrok.com/docs/network-edge/gslb/) (GSLB) on top of a VM-based deployment on Linode.

GLSB improves the performance and resiliency for your apps by distributing traffic to the nearest Point of Prescence (PoP) and the upstream service. Unlike a traditional GSLB deployment, ngrok automatically routes traffic without requiring you to deploy new infrastructure, provision IPs, or change DNS records. All you have to do is configure your ngrok agents for a simple path to high availability, horizontal scaling, A/B testing, and more.

ngrok's GSLB even works on top of Linode's internal load balancing mechanism, NodeBalancer, for additional global resiliency and latency improvements.

:::caution This how-to guide requires:

1. An [ngrok account](https://ngrok.com/signup)
1. An account with [Linode](https://login.linode.com/signup)
1. Three Ubuntu 24.04 virtual machines (VMs) in three globally-distributed regions (e.g. Atlanta, Osaka, and Frankfurt):
   - Hostnames should be unique, ideally using the location of the datacenter (e.g. `atlanta`, `osaka`, and `frankfurt`).
   - [Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/) installed on each VM.
     :::

## **Step 1**: Reserve your ngrok domain {#reserve-domain}

You'll use the [ngrok API](https://ngrok.com/docs/api/) throughout this guide to configure your global load balancer. For a more visual experience, you can accomplish most of the same steps directly through the [ngrok dashboard](https://dashboard.ngrok.com/).

Your first task is to [generate a new API key](https://dashboard.ngrok.com/api) in the ngrok dashboard. Make sure you save the API key before you close the modal, because it won't be displayed again&mdash;for the purposes of this guide, you can export the API key on your local workstation for authenticating requests to the ngrok API.

```bash
export NGROK_API_KEY=<YOUR-API-KEY>
```

Export a variable for your new domain, which will be used a few API calls below. With ngrok, you can reserve any subdomain that ends in an [ngrok-managed domain](https://ngrok.com/docs/network-edge/domains-and-tcp-addresses/) like `.ngrok.app`, `.ngrok.dev`, `.ngrok.pro`, or even `.ngrok.pizza`.

Export a new variable for the domain of your choosing.

```bash
export NGROK_DOMAIN=<YOUR-NGROK-DOMAIN>
```

Create your domain on the [`/reserved_domains` endpoint](https://ngrok.com/docs/api/resources/reserved-domains/) using the two variables you've already exported.

```bash
curl \
  -X POST \
  -H "Authorization: Bearer ${NGROK_API_KEY}" \
  -H "Content-Type: application/json" \
  -H "Ngrok-Version: 2" \
  -d '{"description":"Linode Load Balancing","domain":"'${NGROK_DOMAIN}'"}' \
  https://api.ngrok.com/reserved_domains
```

:::tip Custom domains within ngrok

Using CNAMEs, ngrok can host an endpoint on a domain name you own while also managing the complete TLS certificate lifecycle on your behalf. If you'd prefer to use a custom domain rather than a subdomain at `ngrok.app`, [follow our guide](https://ngrok.com/docs/guides/how-to-set-up-a-custom-domain/) and replace your full custom domain in the `export` command above.

:::

## **Step 2**: Create your ngrok Edge {#create-edge}

With your domain reserved, you can create an [ngrok Edge](https://ngrok.com/docs/network-edge/edges/) to manage all your endpoints and tunnels with a single configuration.

Send a `POST` request to the [`/edges/https` endpoint](https://ngrok.com/docs/api/resources/edges-https/), replacing the `description` field below as needed.

```bash
curl \
  -X POST \
  -H "Authorization: Bearer ${NGROK_API_KEY}" \
  -H "Content-Type: application/json" \
  -H "Ngrok-Version: 2" \
  -d '{"description":"Linode HTTPS Edge","hostports":["'${NGROK_DOMAIN}':443"]}' \
  https://api.ngrok.com/edges/https
```

A successful `201` response from the ngrok API looks like the following:

```json
{
	"id": "edghts_2gp3pEq55Gk1XBln8Hp9EZzRvkn",
	"description": "Linode HTTPS Edge",
	"created_at": "2024-05-22T12:27:52Z",
	"uri": "https://api.ngrok.com/edges/https/edghts_2gp3pEq55Gk1XBln8Hp9EZzRvkn",
	"hostports": ["linode-vps-gslb.ngrok.app:443"],
	"mutual_tls": null,
	"tls_termination": null,
	"routes": []
}
```

Export the `id` in this response, which begins with `edghts_...`, for future use.

```bash
export EDGE_ID=edghts_...
```

## **Step 3**: Create Tunnel Group backends for your VMs {#create-tunnel-group-backends}

Next, create a Tunnel Group for each of your globally-distributed VMs. A Tunnel Group uses one or more labels to identify which agent-created tunnels it should attach to a given Edge and route, which you'll create in a moment.

You'll need to run the API request to the [`/backends/tunnel_group` endpoint](https://ngrok.com/docs/api/resources/tunnel-group-backends/) for **each** of your VMs, changing the `<LOCATION_NAME>` with the name of the city or region of the data center where they're deployed (e.g. `atlanta`, `osaka`, and `frankfurt`).

```bash
curl \
  -X POST \
  -H "Authorization: Bearer ${NGROK_API_KEY}" \
  -H "Content-Type: application/json" \
  -H "Ngrok-Version: 2" \
  -d '{"labels":{"dc":"<LOCATION_NAME>"}}' \
  https://api.ngrok.com/backends/tunnel_group
```

Export the `id`, beginning with `bkdtg_...`, returned from the ngrok API for each Tunnel Group you create.

```bash
export BACKEND_ID_O1=bkdtg_...
export BACKEND_ID_02=bkdtg_...
export BACKEND_ID_03=bkdtg_...
```

## **Step 4**: Create a Weighted Backend and Route on your Edge {#create-weighted-backend}

With a Weighted Backend, you can specify custom weights for any number of associated Tunnel Group backends, shaping precisely how to load-balance traffic across the infrastructure you've distributed around the globe. Without a Weighted Backend, the ngrok Edge and [GSLB](https://ngrok.com/docs/network-edge/gslb/) will always favor the Point of Presence (PoP) and VM nearest the user making requests.

Create a Weighted Backend at the [`/backends/weighted` endpoint](https://ngrok.com/docs/api/resources/weighted-backends/) using your Tunnel Group backend `id`s exported from the previous step. The below command weights your backends equally, but you can use any integer between `0-10000` for precise load balancing configurations.

```bash
curl \
	-X POST https://api.ngrok.com/backends/weighted \
	-H "Authorization: Bearer ${NGROK_API_KEY}" \
	-H "Content-Type: application/json" \
	-H "Ngrok-Version: 2" \
	-d '{"backends":{"'${BACKEND_ID_01}'":1,"'${BACKEND_ID_02}'":1,"'${BACKEND_ID_03}'":1}}'
```

Export the `id` of your new Weighted Backend.

```bash
export WEIGHTED_BACKEND_ID=bkdwd_...
```

You can now create an catch-all Edge Route at `/` using your Weighted Backend using the [`/edges/https/{EDGE_ID}/routes` endpoint](https://ngrok.com/docs/api/resources/edges-https-routes/).

```bash
curl \
  -X POST \
  -H "Authorization: Bearer ${NGROK_API_KEY}" \
  -H "Content-Type: application/json" \
  -H "Ngrok-Version: 2" \
  -d '{"match":"/","match_type":"path_prefix","backend":{"enabled":true,"backend_id":"'${WEIGHTED_BACKEND_ID}'"}}' \
  "https://api.ngrok.com/edges/https/${EDGE_ID}/routes"
```

You now have an active domain, Edge, Tunnel Group backends, a Weighted Backend, and a route. If you open a new tab and navigate to `https://<YOUR_NGROK_DOMAIN>` right now, you'll see an error message about having established an ngrok domain and Edge, but no ngrok agents or active tunnels.

![An example of an ngrok domain and Edge with no matching tunnels](img/edge-no-tunnels.png)

You can also navigate to the [Edges dashboard](https://dashboard.ngrok.com/cloud-edge/edges) to see your Edge configuration, on the domain you reserved, with no online tunnels.

![alt text](img/edge-dashboard-no-tunnels.png)

## **Step 5**: Install the ngrok agent and an example workload on each VM {#install-ngrok-agent}

To get some tunnels online and quickly see how ngrok's GSLB works for the purposes of this guide, you'll use an [example API deployment](https://github.com/joelhans/ngrok-vps-gslb-demo). This demo deployment has four parts:

1. A very simple Go-based API with a single endpoint at `/api`, which returns a randomly-generated UUID and the `hostname` of the machine, which should reflect the regions of your VMs.
2. A `Dockerfile` for containerizing said Go-based API.
3. A `docker-compose.yml` file for starting the API container and a containerized edition of the ngrok agent on the same network.
4. A `ngrok.yml` [agent configuration file](/docs/agent/config.mdx) for connecting the agent to your ngrok account and one of the Tunnel Group backends you previously created.

Repeat the following steps on **each** VM:

1. Clone the demo repository.

   ```bash
   git clone https://github.com/joelhans/ngrok-vps-gslb-demo
   cd ngrok-vps-gslb-demo
   ```

2. Edit the `ngrok.yml` [ngrok agent configuration file](https://ngrok.com/docs/agent/config/) with your `<YOUR_NGROK_AUTHTOKEN>` and the `<LOCATION_NAME>` for that VM, matching the labels you created in [step 3](#create-tunnel-group-backends).

   ```yaml
   version: 2
   authtoken: <YOUR-NGROK-AUTHTOKEN>
   log_level: debug
   log: stdout
   tunnels:
     vps-demo:
       addr: 5000
       labels:
         - dc=<LOCATION_NAME>
   ```

3. Build and start the containerized API deployment, passing the hostname of the VM to the hostname of the Docker container.

   ```bash
   HOSTNAME=$(hostname -f) docker compose up -d
   ```

## **Step 6**: Test out ngrok's Global Server Load Balancing {#test-gslb}

ngrk is now load-balancing your single API endpoint across all three distributed VMs.

You can now ping your demo API at `<YOUR_NGROK_DOMAIN>/api` to see which backend responds.

```bash
curl <YOUR_NGROK_DOMAIN>/api
[{"id":"1cf26269-ce8b-4f16-91f1-fdf7bc6d9e80","dc":"atlanta"}]
```

To see the weighting in action, try a batch of `curl` requests.

```
for i in `seq 1 20`; do \
  curl  \
    -X GET https://<YOUR_NGROK_DOMAIN>/api ; \
  done
```

You should see each VM respond with similar frequency:

```json
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"c46bdbd9-b588-4ffa-8018-d299d0918bbc","dc":"osaka"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
[{"id":"ecc6451c-bc25-43bd-8cb6-a162f019adf3","dc":"frankfurt"}]
[{"id":"dce5a46e-15af-4479-a073-8fbc7f0097c5","dc":"atlanta"}]
```

To confirm your global load balancing, head over to the [Edges dashboard](https://dashboard.ngrok.com/cloud-edge/edges) once again to confirm your tunnels are online and weighted equally at 33.33%.

![The ngrok dashboard showing online, weighted tunnels](img/edge-weighted-active.png)

## **Step 7** (optional): Enable the Traffic Policy module for API gateway features {#enable-traffic-policy}

Your demo API is already globally load-balanced, but if you want to extend your ngrok usage even further, you can enable one or more [Traffic Policy modules](/docs/http/traffic-policy/index.mdx) on your Edge to control traffic or establish standards for how your upstream services are accessed.

You have two options:

- Using `curl` with the ngrok API's [`/edges/https/{edge_id}/routes/{id}/policy` endpoint](/docs/api/resources/edge-route-policy-module.mdx). For this, you will need find the `id` of the catch-all Route you created in [step 4](#create-weighted-backend).

  ```bash
  export ROUTE_ID=edghtsrt_...

  curl \
    -X PUT "https://api.ngrok.com/edges/https/${EDGE_ID}/routes/${ROUTE_ID}/policy" \
    -H "Authorization: Bearer ${NGROK_API_KEY}" \
    -H "Content-Type: application/json" \
    -H "Ngrok-Version: 2" \
    -d '{"enabled":true,"outbound":[{"actions":[{"type":"add-headers","config":{"headers": {"is-ngrok": "1","country": "${.ngrok.geo.country_code}"}}}],"name":"Add ngrok headers"}]}'
  ```

- In the Traffic Policy module section of your [Edge](https://dashboard.ngrok.com/cloud-edge/edges) in the ngrok dashboard, where you can click the **Edit Traffic Policy** button and add the YAML below.

  ```yaml
  inbound: []
  outbound:
    - expressions: []
      name: Add ngrok headers
      actions:
        - type: add-headers
          config:
            headers:
              country: ${.ngrok.geo.country_code}
              is-ngrok: "1"
  ```

When you request your API again, the response includes the `country` (with the appropriate code for where you're generating said request) and `is-ngrok` headers.

```bash
curl -s -I -X GET https://<YOUR_NGROK_DOMAIN>/api

HTTP/2 200
content-type: application/json
country: US
date: Wed, 22 May 2024 14:53:42 GMT
is-ngrok: 1
content-length: 63
```

## What's next?

You now have a globally load-balanced API deployment using 3 Linode VMs, 3 ngrok agents, 3 tunnels, 1 edge, and a single convenient endpoint for your users.

From here, you have many options for extending your use of ngrok's GSLB:

- Create additional deployments in more regions, adding a new Tunnel Group backend for each and updating your Weighted Tunnel configuration, to spread the load even further.
- Add more VMs to a region with existing deployments and add them to the relevant Tunnel Group backend. Ngrok will then load-balance between those specific VMs equally _after_ weighting requests on the Backend level.
  - Alternatively, use [NodeBalancers](https://www.linode.com/docs/products/networking/nodebalancers/) for
- Provision a Kubernetes cluster with the same workload and the ngrok [Kubernetes Operator](/docs/k8s/index.mdx) to load-balance between VM- and Kubernetes-based deployments of the same API or application.
- Use your Weighted Backend for A/B tests by changing your deployments between different regions and VMs.

If you're eager to learn more about ngrok's GSLB, give the following a read:

- [Introducing Always-On Global Server Load Balancing](https://ngrok.com/blog-post/what-is-gslb-global-server-load-balancing)
- [What is global server load balancing (GSLB)?](https://ngrok.com/blog-post/what-is-gslb-global-server-load-balancing)
